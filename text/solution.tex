\chapter{PCI Express}

The \ac{PCIe} is the interconnect standard that emerged in 2003 from the \ac{PCI} standard. The
\ac{PCIe} addressed the drawbacks of the \ac{PCI} that became increasingly challenging when more
devices and more throughput were required. These include:

\begin{itemize}
        \item Shared-bus topology that, inspite of its simplicity, was rendered unscalable since with
        increasing amount of devices, the bus gets longer which weakens the signal for the farthest
        device.
        \item Common clock which does not fit well with parallel data transmission model especially
        when lane skew takes place, meaning the bits of data travel unevenly fast over the wires.
        \item Half-duplex communication allows only one device to communicate at one point at a
        time.
        \item Reflected-wave signaling which was used to reduce the power consumption on the bus.
        This uses an interference of the sent wave from the master (which had driven the signal to
        the lower voltage than needed for the digital circuits on the bus) and its reflection that
        originated on the end of a bus (there is no termination on the bus' end). By the addition of
        these two waves, the signal reaches the required voltage level for the digital circuits (W:
        cite PCIe book).
\end{itemize}

% TODO: Add picture of PCI topology with some functions and one PCI-to-PCI bridge.

The PCI protocol did not keep pace with increasing demand of computer peripheral communication
structures and needed to be changed. The PCI Express protocol significantly changed the strategy for
both the communication topology and the physical properties of the interconnect. 

\section{Communication model}

The basic architecture is shown in Fig. XXX (host topology). Connections are implemented as
point-to-point links between ports on different device types organized in a tree. The center of the
topology is the \ac{rcpx}, which provides \emph{Root Ports}. Each device communicating over the
infrastructure (such as today's GPU/FPGA accelerators) is called \emph{Function} and is the major
source of traffic within the system\footnotemark. Each Function contains an \emph{Upstream port} for
exchanging data. Since the number of Root Ports is limited by the processor's interconnect, fan-out
is provided by \emph{PCIe Switches}, which supply multiple \emph{Downstream ports} and a single
Upstream port. Each port supports full-duplex communication and introduces asynchronous clocking
model.  Using this model significantly improves the possible throughput of the interconnect where
clock gets reconstructed from the patterns in the incoming data on the receiving port. Using
Switches, the number of devices can be increased arbitrarily, making PCIe a switched network (W:
cite PCIe spec). Although the PCIe specification introduces many advanced features, this thesis
presents mostly the necessary ones for its purposes. 

\footnotetext{
    The PCIe specification (with concordance to the PCI) defines three \emph{System elements} to
    identify its devices, namely \emph{Bus}, \emph{Device} and \emph{Function}. Every point-to-point
    connection in the topology makes a separate Bus together with internal (virtual) buses of the
    Switches and the Root Complex. The Device identifier stays as a remnant of the PCI standard and
    every Bus within the PCIe topology contains only one. Within each Device, one or multiple Functions
    can be present. The Function is basically and adressable entity in the configuration space where
    \emph{Endpoint} is the most important type of a Function. Using the term `device' in this thesis
    will refer to a general device in the PCIe topology that can contain one or multiple Functions
    (or Endpoints).
}

% NOTE: Maybe mention BDF identifier if necessary

Devices communicate using units called \acp{TLP}, forming a packetized transfer. Each \ac{TLP}
contains a PCIe header used for identification followed by an optional payload. There are many
\ac{TLP} types, but the ones mentioned in this work are \ac{MRd} and \ac{MWr}. Requests may require
a response (called \emph{non-posted}, as with \acp{MRd}) in the form of \ac{Cpl} presenting a third
\ac{TLP} type. Such requests are called \emph{non-posted} (like \acp{MRd}) while requests that do
not require a response are called \emph{posted} (like \acp{MWr}). The device creating a request is
called \emph{Requester}, while the device `completing' the request (sending a response or just
accepting the payload) is called \emph{Completer}.

The \ac{rcpx} is located on the CPU die and tightly integrated with the processing cores to provide
fast access to the system's I/O devices. The \ac{rcpx} also provides the connection to host memory,
which is the primary communication path used by DMA engines for \ac{H2D} and \ac{D2H} transfers.
Multiple Root Ports or Downstream ports enable direct \ac{D2D} communication, as shown in Fig.\ XXX.

% TODO: Figure with sketched DMA transport with Peer-to-peer transport
% NOTE: Maybe include the layer model if we are going to talk about credits somewhere

\section{Link configuration}

The PCIe specification gets published in versions called \emph{generations}. With every new
specification, the throughput of the bus gets doubled while keeping backward and forward
compatibility between its standards (this is an important feature of PCIe). This is the reason, why
the physical implementation of the protocol stack remains so complicated, especially on the physical
layer. The physical connection between two ports consists of multiple differential pairs called
\emph{lanes} that are bundled together into a \emph{link}. Each lane allows for full-duplex
communication, where each direction is communicated using a separate differential pair. The link can
either contain 1, 2, 4, 8 or 16 lanes.

As for the physical attachment interface, this thesis works with two of them, namely \ac{CEM} and
\emph{M.2}. The \ac{CEM} standard is the oldest one used for \emph{Add-in Cards} with GPU chips,
FPGA accelerators or \acp{NIC}. It supports from 1 to 16 lanes with with wider link widths
supporting lower ones and vice versa (these however with specially carved connectors that fit longer
connector of the attached card). This form-factor allows for connection of the most power consuming
peripherals where the connector can supply up to 75~W. The accelerators can require additional power
by introducing external power supply attachements. The most common ATX connector adds another 75~W
for its 6-pin configuration or 150~W for the 8-pin configuration (W: cite Intel's ATX standard).

% TODO: Image of different CEM connectors on the motherboard

The second type of physical attachement, \emph{M.2}, is designed for ultra-light, power
efficient platforms such as wireless modules or (as in the case of this thesis) \ac{NVMe} drives.
This connector has a fixed set of 4 lanes and a variable module length ranging from 30~mm to 110~mm.
These connectors present a detachable form factor for PCIe links but the standard allows for
permanently attached devices to be connected as well. 

% NOTE: maybe an overview table of available speeds of PCIe nowadays

As it is obvious from the variety of link configurations in terms of speeds and the amount of lanes,
the multitude of PCIe devices inside a compute node needs to agree on common parameters in order to
facilitate data transfer. This process of negotiation is called \emph{link training} and it takes
place after the network get powered up. The training is done on per-link-basis so there are multiple
differently configured links in the system at a given point in time. This is presented on Fig. XXX
where the M.2 SSD needs to communicate with the processor (where Root Ports can usually utilize up
to 16 lanes) or with a \ac{NIC} which supports up to 16 lanes. Other than link's lane count and
transfer rate, other configured parameters include lane polarity and lane reversal. Since the
training and the negotiated link parameters are tha matter of the physical layer, the link
properties remain transparent for the user.

% TODO: Figure of the hierarchy where different link widths are visiable

\section{Device configuration}

After the initial link training is done and every link is active, the host system does device
discovery and configuration upon system boot in order to map the network. The process of discovery
is called \emph{enumeration} and involves a CPU communicating through the Root Complex in order to find
every connected device in the topology and assign its BDF identifier (this includes assigning
numbers also to buses between ports that do not necessarily connect to an Endpoint, like two
Switches or a Switch with the Root Complex). The search is done in a depth-first way where each
device is registered if it responds with a valid \emph{Vendor ID} that is located within device's
configuration registers. After assigning all BDFs in the topology, the configuration software
further configures registers on the discovered devices. The configuration is done solely by the Root
Complex in order to avoid the complexity of management when multiple devices (e.g. Endpoints) would
try to configure each other. The initial assigning of configuration attributes is done using an
IO-based transfer but further setting is done using \ac{MMIO}. 

Each Endpoint contains configuration registers in a 64-byte \emph{PCI Configuration Header}, which
holds the most important attributes controlling the Function's ability to initiate transactions over
PCIe and to be addressable within a host system. Firstly, since devices cannot generate Requests on
their own by default, the \emph{Bus Master} attribute enables this feature. Secondly, in order for
adjacent devices to locate the current Function and to access its internal, user-defined
registers/memory space, at least one of the \acp{BAR} must be initialized.  By using BARs, each
Endpoint requests a memory space within a host system for MMIO access. This is needed to access the
application logic from the outside since each Endpoint accepts only those transactions whose
addresses are in the ranges specified by its BARs. Each Function can register up to six 32-bit BARs
which provides memory segmentation needed by some host applications. When 64-bit addressing is
requested, two consecutive BARs are used. Throughout this work, the observation has been made that
most devices allocate one to two BARs and, when not exceeding 4 GiB of space, an operating system
maps them to 32-bit address ranges even when 64-bit addressing has been requested. First of all,
this addressing has to be enabled in the UEFI settings of the host system.

Besides than the sheer capability for devices to be localizable in the system, other parameters are
configured and need to be abided by by the user applications (e.g.\ the user-defined FPGA logic). A
function defines its supported \ac{MPS} in the \emph{Device Capabilities register}. Seeing this
value, the configuration software sets its preferred value in a field in the \emph{Device Control
regiser}. The value of this field prohibits the Transmitter to dispatch TLP with greater payload
than MPS specified and the Receiver to process them. The configuration allows furter adjustement
using the \ac{MRRS} in the \emph{Device Control register} which prohibits the Transmitter to
dispatch a Read Request greater than the specified size. Available values for these two parameters
range from 128 to 4096~bytes (in powers of two steps). The configuration software balances these
parameters based on latency/bandwidth requirements in the network. The configuration has to take
account of the pairs of adjacent devices communicating with each other since receiving a TLP on a
function's port whose size is larger than its set MPS results in the function rejecting this packet
and reporting a fatal error\footnotemark. 

\footnotetext{The PCIe standard does not require System Elements to perform segmentation of large
    TLPs.} 

\section{Peer-to-peer transfer}

An important aspect of PCIe (with its predecessor as well) and the structure of its network allowed
to introduce \ac{DMA} that allows to copy large amounts of data withouth the host CPU facilitating
such copying. The usual approach it the exchange of data between PCIe devices and the host CPU
through buffers in the host memory. However, the network infrastructure to address other devices in
the topology as well. This type of \ac{D2D} transport is called \ac{P2P} or \emph{Host Bypassing}
and makes a fundamental part of this thesis. There are several implications of running such
transport in the host system. The specification makes the support for each Switch mandatory (for
transport between its Downstream ports) but optional for the Root Complex (for transport between its
Root Ports) (source to spec). The routing of TLP differes based on its type with \acp{MWr} and
\acp{MRd} being routed by memory address while \acs{Cpl} being routed by Requester's BDF identifier.

Today's host CPUs untilize \ac{IOMMU} to translate physical address to virtual ones in order to
securely split I/O peripherals into groups. This unit is crucial in multi-tenant environments (like
guests as virtual machines) where only devices in one group can perform P2P transfers between each
other. Otherwise, where communication between two groups is required, the data need to be analyzed
by the CPU. Without IOMMU a potentially malicious device can write and read from every region in the
host system. The unit works hand in hand with the PCIe's \ac{ACS} which forces every traffic to pass
through the \ac{rcpx} even if the devices are connected to the single Switch (depicted in Fig XXX).
Otherwise, all devices under each Root Port are put to a common IOMMU group which does not play well
with the need to decide isolation on per-device basis.

Considering the test server used in this work, the support for P2P transferes has been discovered as
well as the presence of multiple \acp{IOMMU} and enabled \ac{ACS} on the Root Ports. Since security
is not a topic of this thesis nor are the system considerartions in the multi-tenant environments,
both of these funcionalities have been disabled in server's UEFI. Therefore, the PCIe devices
communicate using physical addresses.

\chapter{Integrated core for PCIe on FPGAs}

Modern FPGAs provide hardened IP core that implements the whole PCIe protocol stack (generally
implementing a Port) that can be configured either as a Root Port (not as a \ac{rcpx} though) or an
Upstream Port (mostly for Endpoint, sometimes for Switch on some devices). The IP utilizes
high-speed serial transceivers located on the edge of the chip that connect directly to the
CEM-compliant Edge connector on the PCB's side. This work utilizes the \emph{Alveo U55C} data-center
acceleration card which is equipped with \verb+xcu55c-fsvh2892-2L-e+ chip (this corresponds to the
serial chip \verb+vu57p+). The available integrated core called \emph{PCIE4C} allows to configure
the interface to run on Gen3 transfer rat on 16 lanes (Gen3x16) or Gen4 transfer rate on 8
lates (Gen4x8) providing the same throughput but reducing logic utilization. Since two of
these cores are provided close to each other and a total of 16 lanes is provided, these cores can
split the available lanes and the acceleration card can be configured to run in a \emph{PCIe
bifurcation} mode. This mode utilizes a single PCIe link running two physically separated devices
(each containing its own set of Functions). Therefore, the acceleration card can reach a cummulative
data trasfer rate of PCIe Gen4x16.

\section{Interfaces to user logic}

Since the described core implement the whole PCIe protocol stack (meaning from Physical
to Transaction layer), the user logic does not have to deal with task of the lower layers, such as
transaction buffering, flow control credit management, data integrity check, lane deskew or clock
recovery. In terms of the PCIe topology, this core implements an Upstream port (specifically as a
PCIe Endpoint). The core provides four separate interfaces for the user logic where each follows the
\emph{AXI4-Stream} specification. As mentioned before, each PCIe port allows for full-duplex
communication, allowing each device to act both as a Requester and as a Completer at the same time.
The core contains these separate interfaces exactly in concordance with this behavior which include:

\begin{description}
    \item[\ac{RQ}] allows user logic to send requests to other components in the PCIe
    hierarchy.
    \item[\ac{rcpl}] transfers completions for non-posted requests dispatched on the RQ
    interfaces.
    \item[\ac{CQ}] receives requests from other devices in the PCIe hierarchy and provides
    them for the user logic.
    \item[\ac{CC}] is used by the user logic to send responses for non-posted requests
    received on the CQ interface.
\end{description}

Other than that, the core provides additional interfaces for transferring status information either
from the link or from user logic, dispatching and receiving interrupts, sending messages, etc. The
PCIe IP maintains its set of status registers as every other PCIe Endpoint that are accessible by
an operating system. A separate interface for the Requester side can be used to manage flow control
credits but this is not used in this work and the management is left on the PCIe IP.

% TODO: Mention somewhere the addressing by DWords

The width of the AXI4-Stream interfaces depends on the configured transfer rate of the IP. This work
implements the DMA engine using 8-lane Gen4 PCIe endpoint which results in a 512-bit wide bus
running on 250 MHz. All of the interfaces are running on thesame clock that is provided as the
`user clock' output from the IP (cite PCIe IP spec). Data units communicated on these buses are
adjusted \acp{TLP} where every is formed as a \emph{TLP header} followed by a payload of user data.
The header uniquely identifies a payload within the PCIe topology with a physical address to the
reserved space in the host memory or in another PCIe BAR. TLP header has a fixed size of 3~DW (12~B) for
RC/CC interfaces and 4~DW (16~B) for RQ/CQ interfaces.

Using wide buses carries a significant drawback when data do not fill whole bus words. For example,
transferring 1~B of data using a 512b bus utilizes only about 1.5~\% of the bandwidth (1~B of payload
plus a 16~B TLP header). To mitigate this inefficiency, the bus word is separated into two segments
of equal size. Each segment can contain up to one TLP start and/or one TLP end. This doubles the
throughput for packets that fit entirely within a single segment and generally improves throughput
when a packet ends in the first segment (the second packet can then begin in the second segment).

% TODO: Describe format of the TLP header

\chapter{Non-volatile Memory Express}

The \ac{NVMe} standard emerged in 2011 as a way of integration of non-volatile storage into the
\ac{PCIe} domain\footnotemark. The standard emerged as a successor of \emph{SAS} and \emph{SATA}
specifications in order to meet the requirements of consumer systems in terms of latency, throughput
and scalability that fits with the flash-based storage. All of these standards introduced a queue
mechanism where commands (like write and read) are present and processed by the controller of the
attached \ac{NVM}. Both, SATA and SAS support one command queue allowing up to 32 and 256 commands
respectively. NVMe enhances this mechanism by allowing to instantiate up to $2^{16}$ queues where
each can contain up to $2^{16}$ commands. Allowing to have many queues results in hardware-aided
parallel access to the \ac{NVM} where, for example, each CPU core receives its own queue. The schema
of the NVMe device is depicted in Fig. XXX. The \emph{NVMe controller} sits on top of a PCIe
controller, managing the instantiation of queues and processing of commands. The non-volatile
storage is presented as a \emph{Namespace} which is the basic referencable unit of storage in the
NVMe standard\footnotemark. The controller is equipped with a DMA engine that performs a read and
write of raw data between memory regions in the host system. This implies that the NVMe device
behaves as a Bus Master and can therefore generate its own \acp{MWr} and \acp{MRd} which is suitable
for the \ac{P2P} communication in the PCIe domain.

\footnotetext{This thesis works with specification version 1.2b since it describes only its basic
    concepts and not every currently working NVMe device supports the newest standard. The NVMe
    specification is designed to be backward compatible though. Using older specifiactions may be
    advised for boarding users to understand the basic concepts quickly.}

\footnotetext{This is a simplified view in the referenced version of the NVMe specification. Later
    versions define a more complex and fine-grained classification beyond namespaces. Moreover,
    there can be many namespaces maintained by a single NVMe controller as well as one namespace shared
    by many controllers.}

The command mechanism provides two types of queues that always occur in pairs, the \emph{Submission
queue} and the \emph{Completion queue}. A Submission queue contains commands that need to be
processed by the NVMe controller which, after a command's execution, publishes a result of an
operation  into a Completion queue. These queues are operated in the FIFO manner and are allocated
as circular buffers in host-addressable memory. The most important pair is the \emph{Admin
queue-pair} that is used for additional configuration. The controller appears in the PCIe system as
a single Function and, initially, the kernel is able to only reach the standard registers in the
PCIe Configuration Space and its \acp{BAR}. The NVMe driver allocates Admin queue-pair and publishes
base addresses and sizes of the queues to controller registers located in BAR0.

For data trasmission to/from the NVMe namespace, the configuration driver instantiates one or more
\emph{I/O queue-pairs} which are used by the consumers/producers of data. Same as for the Admin
queue-pair, these queues are allocated circular buffers in host-addressable memory. A fully
initialized system with a 4-core processor and an NVMe device is shown in Fig. XXX. The controller
can associate multiple I/O Submission queues with one I/O Completion queue in order to save
resources (this is not possible for the Admin queue-pair though since only one can be instantiated). 

% TODO: Figure with 4 IO queue-pairs, 1 Admin queue pair and

% Ordering of commands
% Arbitration between queues
% Where are the queues stored
% Multi-pathing in our case (N FPGAs to M NVMes)
% Depict NVMe Read/Write in the timing diagram

\section{Order of operations}

A producer submits a command to a submission queue by using a template for \ac{SQE} described
further. Upon storing such entry in the queue, the producer updates the \ac{SQTDBL} pointer in
controller registers (in BAR0) in order to signalize the controller that new command has been
submitted. This is followed by the controller fetching and executing the command followed by
dispatching a completion into a completion queue that follows the \ac{CQE} template. If a command
operates with additional data (like reading/writing data from/to the \ac{NVM} or fetching commands
from a submission queue), the controller engages the DMA engine on the device.

% Phase bit to recognize new CQEs

The NVMe controller arbitrates between all of its queues in a Round-robin fashion\footnotemark.

\footnotetext{Advanced devices can implement a Weighted Round-robin or vendor-specific mechanism.}
