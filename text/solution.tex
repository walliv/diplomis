\chapter{PCI Express}

The \ac{PCIe} is the interconnect standard that emerged in 2003 from the \ac{PCI} standard. The
\ac{PCIe} addressed the drawbacks of the \ac{PCI} that became increasingly challenging when more
devices and more throughput were required. These include:

\begin{itemize}
        \item Shared-bus topology that, inspite of its simplicity, was rendered unscalable since with
        increasing amount of devices, the bus gets longer which weakens the signal for the farthest
        device.
        \item Common clock which does not fit well with parallel data transmission model especially
        when lane skew takes place, meaning the bits of data travel unevenly fast over the wires.
        \item Half-duplex communication allows only one device to communicate at one point at a
        time.
        \item Reflected-wave signaling which was used to reduce the power consumption on the bus.
        This uses an interference of the sent wave from the master (which had driven the signal to
        the lower voltage than needed for the digital circuits on the bus) and its reflection that
        originated on the end of a bus (there is no termination on the bus' end). By the addition of
        these two waves, the signal reaches the required voltage level for the digital circuits (W:
        cite PCIe book).
\end{itemize}

% TODO: Add picture of PCI topology with some functions and one PCI-to-PCI bridge.

The PCI protocol did not keep pace with increasing demand of computer peripheral communication
structures and needed to be changed. The PCI Express protocol significantly changed the strategy for
both the communication topology and the physical properties of the interconnect. 

\section{Communication model}

The basic architecture is shown in Fig. XXX (host topology). Connections are implemented as
point-to-point links between ports on different device types organized in a tree. The center of the
topology is the \ac{rcpx}, which provides \emph{Root Ports}. Each device communicating over the
infrastructure (such as today's GPU/FPGA accelerators) is called \emph{Function} and is the major
source of traffic within the system\footnotemark. Each Function contains an \emph{Upstream port} for
exchanging data. Since the number of Root Ports is limited by the processor's interconnect, fan-out
is provided by \emph{PCIe Switches}, which supply multiple \emph{Downstream ports} and a single
Upstream port. Each port supports full-duplex communication and introduces asynchronous clocking
model.  Using this model significantly improves the possible throughput of the interconnect where
clock gets reconstructed from the patterns in the incoming data on the receiving port. Using
Switches, the number of devices can be increased arbitrarily, making PCIe a switched network (W:
cite PCIe spec). Although the PCIe specification introduces many advanced features, this thesis
presents mostly the necessary ones for its purposes. 

\footnotetext{
    The PCIe specification (with concordance to the PCI) defines three \emph{System elements} to
    identify its devices, namely \emph{Bus}, \emph{Device} and \emph{Function}. Every point-to-point
    connection in the topology makes a separate Bus together with internal (virtual) buses of the
    Switches and the Root Complex. The Device identifier stays as a remnant of the PCI standard and
    every Bus within the PCIe topology contains only one. Within each Device, one or multiple Functions
    can be present. The Function is basically and adressable entity in the configuration space where
    \emph{Endpoint} is the most important type of a Function. Using the term `device' in this thesis
    will refer to a general device in the PCIe topology that can contain one or multiple Functions
    (or Endpoints).
}

% NOTE: Maybe mention BDF identifier if necessary

Devices communicate using units called \acp{TLP}, forming a packetized transfer. Each \ac{TLP}
contains a PCIe header used for identification followed by an optional payload. There are many
\ac{TLP} types, but the ones mentioned in this work are \ac{MRd} and \ac{MWr}. Requests may require
a response (called \emph{non-posted}, as with \acp{MRd}) in the form of \ac{Cpl} presenting a third
\ac{TLP} type. Such requests are called \emph{non-posted} (like \acp{MRd}) while requests that do
not require a response are called \emph{posted} (like \acp{MWr}). The device creating a request is
called \emph{Requester}, while the device `completing' the request (sending a response or just
accepting the payload) is called \emph{Completer}.

The \ac{rcpx} is located on the CPU die and tightly integrated with the processing cores to provide
fast access to the system's I/O devices. The \ac{rcpx} also provides the connection to host memory,
which is the primary communication path used by DMA engines for \ac{H2D} and \ac{D2H} transfers.
Multiple Root Ports or Downstream ports enable direct \ac{D2D} communication, as shown in Fig.\ XXX.

% TODO: Figure with sketched DMA transport with Peer-to-peer transport
% NOTE: Maybe include the layer model if we are going to talk about credits somewhere

\section{Link configuration}

The PCIe specification gets published in versions called \emph{generations}. With every new
specification, the throughput of the bus gets doubled while keeping backward and forward
compatibility between its standards (this is an important feature of PCIe). This is the reason, why
the physical implementation of the protocol stack remains so complicated, especially on the physical
layer. The physical connection between two ports consists of multiple differential pairs called
\emph{lanes} that are bundled together into a \emph{link}. Each lane allows for full-duplex
communication, where each direction is communicated using a separate differential pair. The link can
either contain 1, 2, 4, 8 or 16 lanes.

As for the physical attachment interface, this thesis works with two of them, namely \ac{CEM} and
\emph{M.2}. The \ac{CEM} standard is the oldest one used for \emph{Add-in Cards} with GPU chips,
FPGA accelerators or \acp{NIC}. It supports from 1 to 16 lanes with with wider link widths
supporting lower ones and vice versa (these however with specially carved connectors that fit longer
connector of the attached card). This form-factor allows for connection of the most power consuming
peripherals where the connector can supply up to 75~W. The accelerators can require additional power
by introducing external power supply attachements. The most common ATX connector adds another 75~W
for its 6-pin configuration or 150~W for the 8-pin configuration (W: cite Intel's ATX standard).

% TODO: Image of different CEM connectors on the motherboard

The second type of physical attachement, \emph{M.2}, is designed for ultra-light, power
efficient platforms such as wireless modules or (as in the case of this thesis) \ac{NVMe} drives.
This connector has a fixed set of 4 lanes and a variable module length ranging from 30~mm to 110~mm.
These connectors present a detachable form factor for PCIe links but the standard allows for
permanently attached devices to be connected as well. 

% NOTE: maybe an overview table of available speeds of PCIe nowadays

As it is obvious from the variety of link configurations in terms of speeds and the amount of lanes,
the multitude of PCIe devices inside a compute node needs to agree on common parameters in order to
facilitate data transfer. This process of negotiation is called \emph{link training} and it takes
place after the network get powered up. The training is done on per-link-basis so there are multiple
differently configured links in the system at a given point in time. This is presented on Fig. XXX
where the M.2 SSD needs to communicate with the processor (where Root Ports can usually utilize up
to 16 lanes) or with a \ac{NIC} which supports up to 16 lanes. Other than link's lane count and
transfer rate, other configured parameters include lane polarity and lane reversal. Since the
training and the negotiated link parameters are tha matter of the physical layer, the link
properties remain transparent for the user.

% TODO: Figure of the hierarchy where different link widths are visiable

\section{Device configuration}

After the initial link training is done and every link is active, the host system does device
discovery and configuration upon system boot in order to map the network. The process of discovery
is called \emph{enumeration} and involves a CPU communicating through the Root Complex in order to find
every connected device in the topology and assign its BDF identifier (this includes assigning
numbers also to buses between ports that do not necessarily connect to an Endpoint, like two
Switches or a Switch with the Root Complex). The search is done in a depth-first way where each
device is registered if it responds with a valid \emph{Vendor ID} that is located within device's
configuration registers. After assigning all BDFs in the topology, the configuration software
further configures registers on the discovered devices. The configuration is done solely by the Root
Complex in order to avoid the complexity of management when multiple devices (e.g. Endpoints) would
try to configure each other. The initial assigning of configuration attributes is done using an
IO-based transfer but further setting is done using \ac{MMIO}. 

Each Endpoint contains configuration registers in a 64-byte \emph{PCI Configuration Header}, which
holds the most important attributes controlling the Function's ability to initiate transactions over
PCIe and to be addressable within a host system. Firstly, since devices cannot generate Requests on
their own by default, the \emph{Bus Master} attribute enables this feature. Secondly, in order for
adjacent devices to locate the current Function and to access its internal, user-defined
registers/memory space, at least one of the \acp{BAR} must be initialized.  By using BARs, each
Endpoint requests a memory space within a host system for MMIO access. This is needed to access the
application logic from the outside since each Endpoint accepts only those transactions whose
addresses are in the ranges specified by its BARs. Each Function can register up to six 32-bit BARs
which provides memory segmentation needed by some host applications. When 64-bit addressing is
requested, two consecutive BARs are used. Throughout this work, the observation has been made that
most devices allocate one to two BARs and, when not exceeding 4 GiB of space, an operating system
maps them to 32-bit address ranges even when 64-bit addressing has been requested. First of all,
this addressing has to be enabled in the UEFI settings of the host system.

Besides than the sheer capability for devices to be localizable in the system, other parameters are
configured and need to be abided by by the user applications (e.g.\ the user-defined FPGA logic). A
function defines its supported \ac{MPS} in the \emph{Device Capabilities register}. Seeing this
value, the configuration software sets its preferred value in a field in the \emph{Device Control
regiser}. The value of this field prohibits the Transmitter to dispatch TLP with greater payload
than MPS specified and the Receiver to process them. The configuration allows furter adjustement
using the \ac{MRRS} in the \emph{Device Control register} which prohibits the Transmitter to
dispatch a Read Request greater than the specified size. Available values for these two parameters
range from 128 to 4096~bytes (in powers of two steps). The configuration software balances these
parameters based on latency/bandwidth requirements in the network. The configuration has to take
account of the pairs of adjacent devices communicating with each other since receiving a TLP on a
function's port whose size is larger than its set MPS results in the function rejecting this packet
and reporting a fatal error\footnotemark. 

\footnotetext{The PCIe standard does not require System Elements to perform segmentation of large
    TLPs.} 

\section{Peer-to-peer transfer}

An important aspect of PCIe (with its predecessor as well) and the structure of its network allowed
to introduce \ac{DMA} that allows to copy large amounts of data withouth the host CPU facilitating
such copying. The usual approach it the exchange of data between PCIe devices and the host CPU
through buffers in the host memory. However, the network infrastructure to address other devices in
the topology as well. This type of \ac{D2D} transport is called \ac{P2P} or \emph{Host Bypassing}
and makes a fundamental part of this thesis. There are several implications of running such
transport in the host system. The specification makes the support for each Switch mandatory (for
transport between its Downstream ports) but optional for the Root Complex (for transport between its
Root Ports) (source to spec). The routing of TLP differes based on its type with \acp{MWr} and
\acp{MRd} being routed by memory address while \acs{Cpl} being routed by Requester's BDF identifier.

Today's host CPUs untilize \ac{IOMMU} to translate physical address to virtual ones in order to
securely split I/O peripherals into groups. This unit is crucial in multi-tenant environments (like
guests as virtual machines) where only devices in one group can perform P2P transfers between each
other. Otherwise, where communication between two groups is required, the data need to be analyzed
by the CPU. Without IOMMU a potentially malicious device can write and read from every region in the
host system. The unit works hand in hand with the PCIe's \ac{ACS} which forces every traffic to pass
through the \ac{rcpx} even if the devices are connected to the single Switch (depicted in Fig XXX).
Otherwise, all devices under each Root Port are put to a common IOMMU group which does not play well
with the need to decide isolation on per-device basis.

Considering the test server used in this work, the support for P2P transferes has been discovered as
well as the presence of multiple \acp{IOMMU} and enabled \ac{ACS} on the Root Ports. Since security
is not a topic of this thesis nor are the system considerartions in the multi-tenant environments,
both of these funcionalities have been disabled in server's UEFI. Therefore, the PCIe devices
communicate using physical addresses.

\chapter{Integrated core for PCIe on FPGAs}

Modern FPGAs provide hardened IP core that implements the whole PCIe protocol stack (generally
implementing a Port) that can be configured either as a Root Port (not as a \ac{rcpx} though) or an
Upstream Port (mostly for Endpoint, sometimes for Switch on some devices). The IP utilizes
high-speed serial transceivers located on the edge of the chip that connect directly to the
CEM-compliant Edge connector on the PCB's side. This work utilizes the \emph{Alveo U55C} data-center
acceleration card which is equipped with \verb+xcu55c-fsvh2892-2L-e+ chip (this corresponds to the
serial chip \verb+vu57p+). The available integrated core called \emph{PCIE4C} allows to configure
the interface to run on Gen3 transfer rat on 16 lanes (Gen3x16) or Gen4 transfer rate on 8
lates (Gen4x8) providing the same throughput but reducing logic utilization. Since two of
these cores are provided close to each other and a total of 16 lanes is provided, these cores can
split the available lanes and the acceleration card can be configured to run in a \emph{PCIe
bifurcation} mode. This mode utilizes a single PCIe link running two physically separated devices
(each containing its own set of Functions). Therefore, the acceleration card can reach a cummulative
data trasfer rate of PCIe Gen4x16.

\section{Interfaces to user logic}

Since the described core implement the whole PCIe protocol stack (meaning from Physical
to Transaction layer), the user logic does not have to deal with task of the lower layers, such as
transaction buffering, flow control credit management, data integrity check, lane deskew or clock
recovery. In terms of the PCIe topology, this core implements an Upstream port (specifically as a
PCIe Endpoint). The core provides four separate interfaces for the user logic where each follows the
\emph{AXI4-Stream} specification. As mentioned before, each PCIe port allows for full-duplex
communication, allowing each device to act both as a Requester and as a Completer at the same time.
The core contains these separate interfaces exactly in concordance with this behavior which include:

\begin{description}
    \item[\ac{RQ}] allows user logic to send requests to other components in the PCIe
    hierarchy.
    \item[\ac{rcpl}] transfers completions for non-posted requests dispatched on the RQ
    interfaces.
    \item[\ac{CQ}] receives requests from other devices in the PCIe hierarchy and provides
    them for the user logic.
    \item[\ac{CC}] is used by the user logic to send responses for non-posted requests
    received on the CQ interface.
\end{description}

Other than that, the core provides additional interfaces for transferring status information either
from the link or from user logic, dispatching and receiving interrupts, sending messages, etc. The
PCIe IP maintains its set of status registers as every other PCIe Endpoint that are accessible by
an operating system. A separate interface for the Requester side can be used to manage flow control
credits but this is not used in this work and the management is left on the PCIe IP.

The width of the AXI4-Stream interfaces depends on the configured transfer rate of the IP. This work
implements the DMA engine using 8-lane Gen4 PCIe endpoint which results in a 512-bit wide bus
running on 250 MHz. All of the interfaces are running on thesame clock that is provided as the
`user clock' output from the IP (cite PCIe IP spec). Data units communicated on these buses are
adjusted \acp{TLP} where every is formed as a \emph{TLP header} followed by a payload of user data.
The header uniquely identifies a payload within the PCIe topology with a physical address to the
reserved space in the host memory or in another PCIe BAR. TLP header has a fixed size of 3~DW (12~B) for
RC/CC interfaces and 4~DW (16~B) for RQ/CQ interfaces.

Using wide buses carries a significant drawback when data do not fill whole bus words. For example,
transferring 1~B of data using a 512b bus utilizes only about 1.5~\% of the bandwidth (1~B of payload
plus a 16~B TLP header). To mitigate this inefficiency, the bus word is separated into two segments
of equal size. Each segment can contain up to one TLP start and/or one TLP end. This doubles the
throughput for packets that fit entirely within a single segment and generally improves throughput
when a packet ends in the first segment (the second packet can then begin in the second segment).

\chapter{Non-volatile Memory Express}

The \ac{NVMe} standard emerged in 2011 as a way of integration of non-volatile storage into the
\ac{PCIe} domain\footnotemark. The standard emerged as a successor of \emph{SAS} and \emph{SATA}
specifications in order to meet the requirements of consumer systems in terms of latency, throughput
and scalability that fits with the flash-based storage. All of these standards introduced a queue
mechanism where commands (like write and read) are present and processed by the controller of the
attached \ac{NVM}. Both, SATA and SAS support one command queue allowing up to 32 and 256 commands
respectively. NVMe enhances this mechanism by allowing to instantiate up to $2^{16}$ queues where
each can contain up to $2^{16}$ commands. Allowing to have many queues results in hardware-aided
parallel access to the \ac{NVM} where, for example, each CPU core receives its own queue. The schema
of the NVMe device is depicted in Fig. XXX. The \emph{NVMe controller} sits on top of a PCIe
controller, managing the instantiation of queues and processing of commands. The non-volatile
storage is presented as a \emph{Namespace} which is the basic referencable unit of storage in the
NVMe standard\footnotemark. A namespace is split into multiple sectors marked by \acp{LBA} which are
at least 512~B in size (which is mostly used one). The controller is equipped with a DMA engine that
performs a read and write of raw data between memory regions in the host system.  This implies that
the NVMe device behaves as a Bus Master and can therefore generate its own \acp{MWr} and \acp{MRd}
which is suitable for the \ac{P2P} communication in the PCIe domain.

\footnotetext{This thesis works with specification version 1.2b since it describes only its basic
    concepts and not every currently working NVMe device supports the newest standard. The NVMe
    specification is designed to be backward compatible though. Using older specifiactions may be
    advised for boarding users to understand the basic concepts quickly.}

\footnotetext{This is a simplified view in the referenced version of the NVMe specification. Later
    versions define a more complex and fine-grained classification beyond namespaces. Moreover,
    there can be many namespaces maintained by a single NVMe controller as well as one namespace shared
    by many controllers.}

The command mechanism provides two types of queues that always occur in pairs, the \emph{Submission
queue} and the \emph{Completion queue}. A Submission queue contains commands that need to be
processed by the NVMe controller which, after a command's execution, publishes a result of an
operation  into a Completion queue. These queues are allocated as circular buffers in a
host-addressable memory. The most important pair is the \emph{Admin queue-pair} that is used for
additional configuration. The controller appears in the PCIe system as a single Function and,
initially, the kernel is able to only reach the standard registers in the PCIe Configuration Space
and its \acp{BAR}. The NVMe driver allocates Admin queue-pair and publishes base addresses and sizes
of the queues to controller registers located in BAR0.

For data trasmission to/from the NVMe namespace, the configuration driver instantiates one or more
\emph{I/O queue-pairs} which are used by the consumers/producers of data. Same as for the Admin
queue-pair, these queues are allocated circular buffers in host-addressable memory. A fully
initialized system with a 4-core processor and an NVMe device is shown in Fig. XXX. The controller
can associate multiple I/O Submission queues with one I/O Completion queue in order to save
resources (this is not possible for the Admin queue-pair though since only one can be instantiated).
Each queue pair is identified by its ID where Admin queue-pair has a fixed ID of 0 and every other
I/O queue pair reserves an ID of a higher index.

% TODO: Figure with 4 IO queue-pairs, 1 Admin queue pair and

% Where are the queues stored
% Multi-pathing in our case (N FPGAs to M NVMes)
% Depict NVMe Read/Write in the timing diagram

\section{Principles of operation}

One queue 2 pointers for its operation, a \emph{Head doorbell} and a \emph{Tail doorbell}. A
consumer of entries from the queue operates a Head doorbell whereas every producer operates a Tail
doorbell. This pair of pointers ensures that a producer does not overwrite unprocessed entries and
that a consumer reads only valid ones. A queue is considered empty if values of these pointers are
equal, and full if a value of the Tail doorbell is equal the value of the Head doorbell minus one
modulo a size of the queue as is shown in Fig. XXX.

% TODO: Figure with shown empty queue and multiple full queue conditions.
% Mention the effective size of a queue

A master submits a command to a submission queue by using a template for \ac{SQE} described further.
Upon storing such entry in the queue, the master updates the \ac{SQTDBL} pointer in controller
registers (in BAR0) in order to signalize the controller that new command has been submitted. This
is followed by the controller fetching and executing the command followed by dispatching a
completion into a completion queue that follows the \ac{CQE} template. If a command operates with
additional data (like reading/writing data from/to the \ac{NVM} or fetching commands from a
submission queue), the controller engages the DMA engine on the device. To detect a received
completion of a command, the master does not have to read the \ac{CQTDBL} value (since this is not
available and remains internal to the NVMe controller anyways). Rather, every \ac{CQE} contains a
\emph{Phase tag (P)} that indicates that a position in the Completion queue contains a valid entry.

\subsection{Command submission}

The \ac{SQE} is created by a master by following a format in Fig. XXX (the grey marked fields are
either unused or reserved) and it uniquely defines a command within a single NVMe controller. The
entry has a length of 64~B (16~DW) with necessary information for a controller to process a command.
There are two sets of commands defined by the NVMe specification, namely \emph{Admin Command Set}
and \emph{NVM Command Set}\footnotemark. The mostly used used commands in this thesis on runtime are
read and write from/to the \ac{NVM} that are described in the following paragraph.

An \ac{SQE} begins with the \ac{OPC} field that distinguishes between different types of commands,
the \emph{PSDT} field that specifies that \acp{PRP} are going to be used as copy buffers and the
\ac{CID} field that presents a unique identifier when combined with the Submission Queue ID. This
combination serves as a tag to match the dispatched command with a received completion. The first
command DWord is followed by \ac{NSID} to index a specific namespace that is managed by the adjacent
NVMe controller (the index is 1-based, meaning that the first namespace receives NSID 1).  The
following \ac{DPTR} field serves as a pointer to the host-adressable memory either from which data
need to be fetched (as for the write command) or to which the data are copied from the \ac{NVM} (as
for the read command). The data pointer is split into two \ac{PRP} entries depending on how many
memory pages contain the operated data in host memory, thus how big the transfer is going to be. If
the transfer fits into one page, only \ac{PRP1} is used and it points to the start of the user data.
In case a transfer, thus operated data, spans over 2 memory pages, the \ac{PRP2} is used as well to
point to the second page. If a transfer spans more than 2 pages, the \ac{PRP2} changes its meaning
to point to the \emph{PRP List} which contains pointers to every other page that contains to the
user data. The following two Dwords (DW10 and DW11) contain the \ac{SLBA} field in the namespace the
last usable field in DW12 contains the \ac{NLB} to copy. This size determines a size of a transfer
for a currently processed command. The size of a transfer is limited by the NVMe controller and
every master has to consult controller parameters in order to not submit larger transfers. When
larger data  chunks are required than a transfer can process, multiple commands have to be
submitted.

% Ordering of completions

\footnotetext{In terms of more recent specification revisions, these sets use a \emph{memory-based}
    transport which differs from \emph{message-based} transport. While the first type of transport is
    typical for PCIe, newer specifications allow to attach NVM devices to other fabrics, like RDMA or
    Ethernet, that use the second type of transport. These are part of the  \ac{NVMe-oF} standard
    published separately.} 

\subsection{Command completion}

After a command is processed the controller dispatches a \ac{CQE} to the Completion queue to notify
the master about the status of a command. The template of every entry is shown in Fig. XXX. A
\ac{CQE} is at least 16 bytes in size. The first DWord is used by some commands to publish
command-specific information (the Read and Write commands do not use them though) while the second
DWord is reserved. An NVMe controller indicates the internal value of \ac{SQHDBL} through every
completion entry which notifies the transmitter of commands about how many commands have been read
by the controller (thus providing a backpressure). The \ac{SQID} and \ac{CID} fields serve a unique
identifier of a command in the communication chain. The \ac{SQID} is present because the NVMe
specification allows to associate multiple Submission queue with one Completion queue (this applies
for one NVMe controller though since every controller maintains its own set of queues). A
\emph{Phase Tag (P)} field follows and its value indicates that the current position in the
Completion queue is valid (meaning the position to which \ac{CQHDBL} points to). The value that is
considered valid is flipped upon every \ac{CQTDBL} rollover, starting as logical 1 for the initial
pass through queue (i.e.\ after queue initialization), then flipping to 0 when the pointer rolls
back to 0, then to 1 again and so on.

The last bits of a CQE are occupied by status information about the successful/unsuccessful
execution of a command. The \ac{DNR} bit indicates that if the same command is going to be
resubmitted, it is expected to fail. The submitter has to wait till this bit clears to 0 in order to
receive valid data. The \emph{More (M)} bit indicates that a completion for a command is going to be
composed out of multiple \acp{CQE}. A more specific information about a command's status can be
found in the \ac{SCT} and \ac{SC} fields. A NVMe controller provides coarse-grained category of
status within the first field whereas a more fine-grained in the second one. Here are some examples
of possible status codes received:

\begin{itemize}
    \item A successfull completion is indicated by the SCT field set to 0h (i.e.\ \emph{Generic
        Command Status}) and the SC field set to 00h (i.e.\ \emph{Successful Completion}).
    \item When a submitter dispatches an NVMe command with a specified LBA range that exceeds the
        capacity of the Namespace, the SCT field is set to 00h and the SC field to 80h (i.e.\
        \emph{LBA Out of Range}).
    \item Submitting two NVMe Write/Read commands with the same \ac{CID} results in a completion
        status where SCT is set to 1h (i.e.\ \emph{Command Specific Status}) and SC set to 80h
        (i.e.\ \emph{Conflicting Attibutes}).
\end{itemize}

\subsection{Ordering}

An important note has to be made with regards to ordering of submitted commands. An NVMe controller
arbitrates between its own set of submission queues in a round-robin fashion and can fetch multiple
commands out of every queue\footnotemark. The order of execution where many commands are fetched is not defined
by the NVMe specification and can be arbitrary. This is the reason why a tagging of commands using
the \ac{CID} field is important so the submittier can bound a completion to a dispatched command.
The order of execution can therefore be derived from the order in which the completions for mutliple
commands have been received. If a strict ordering of commands is required, a submitter needs to
implement its own for that. However, by the means of the NVMe specification, there are two ways of
how to ensure ordering. The standard allows to use \emph{Fused operations} in order to force
ordering of two subsequent commands. Such two commands can be executed atomically, resulting in the
success only if both of the commands succeed. However, this feature is optional and, therefore,
implementation specific. The second way that is always available, is to submit only one command to
the submission queue and wait for its completion before submitting another commands. This can be
guarded by the submitter itself or by initializing the submission queue with the size of 2 (since
this can contain at most 1 valid SQE) and using the blocking by the doorbell pointers. The second
way is used in the implementation proposed by this thesis. 

\footnotetext{Advanced devices can implement a Weighted Round-robin or some vendor-specific mechanism.}

\section{Multi-pathing}

Since multiple queues can be initialized for a single NVMe controller, multiple submitters can
execute commands on the controller. Since the proposed implementation in this thesis introduces a
direct NVMe device access from an FPGA acceleration card using the PCIe \ac{P2P} transfer, it contains
a potential to access multiple NVMe devices (the 1-to-M topology) as well as to access multiple NVMe
devices from multiple acceleration cards, thus implementing an N-to-M topology\footnotemark. These
can be seen in Figure XXX and XXX. This feature that allows to access one NVMe device from multipler
masters is sometimes called \emph{multi-pathing}.

\footnotetext{This topoolgy, however, is only virtual since it is made only by addressing and not by
  physical connections. When performance (like throughput) is critical, it should be adapted on the
  underlying PCIe topology which is organized as a tree by design.}
