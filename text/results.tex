\chapter{DMA Iuventus}

Since this thesis proposes a solution that is able to access the filesystem located on an NVMe
device that is shared between the host OS and an FPGA accelerator a specific IP core is needed on
the accelerator's side. Without this, the user logic inside an FPGA would deal with raw PCIe data
not only from the adjacent NVMe device but from the host system as well. Therefore, the \emph{DMA
Iuventus} IP core has been developed to process requests between an accelerator and an NVMe device.
The \ac{DMA} description has been retained to highlight the mechanism of the direct copying of data
to the adjacent storage without a host CPU facilitating such transfers (this is sometimes called
\emph{Host bypassing}). This IP sits on the integrated block for PCIe of the FPGA to accept
NVMe-specific traffic alongside with a logic that processes transactions to \ac{C/S} registers. 

% Processing of read transactions

\section{TLP Headers}

The DMA IP utilizes 3 of the 4 interfaces provided by the integrated block for PCI Express, namely
\ac{RQ}, \ac{CQ}, \ac{CC}. Each on of these uses its own template of the TLP header that the DMA
core needs to create or process. The \ac{RQ} header has a size of 4~DW and its format is depicted in
Fig. XXX. The \emph{Address} field is aligned with respect to DWords since its two bottom bits are
reserved\footnotemark. For \acp{MWr}, the \emph{Dword Count} field specifies the size of the payload
following the header, whereas, for \acp{MRd}, it specifies the required amount of data that a
Completer needs to send in its Completion. The type of a request is defined in the \emph{Request
Type} field. The \emph{Function} field informs the PCIe IP which Function generated the request
(more on that later). The last used field is the \emph{NoSnoop} bit located in the \emph{Attr}
field. Asserting this bit instructs the the host CPU that the underlying address space the
transaction is targeted to, does not require a hardware-enforced cache coherency (cite the PCIe
spec). This means that the address space is not shared by any other device in the system. The
\ac{RQ} interface is used to dispatch \ac{SQTDBL} and \ac{CQHDBL} updates to the NVMe registers.

\footnotetext{The x86\_64 architecture addresses its physical memory space with resolution to bytes.}

The \ac{CQ} header (Fig. XXX) follows a similar format as the \ac{RQ} header where the user logic
acts as a PCIe Completer. The \emph{Target Function} field indicates to which the Function is
targeted. When multiple \acp{BAR} are used, the core puts index of the specific one in the \emph{BAR
ID} field. The size of a targeted BAR is determined by the \emph{BAR Aperture} which serves as a
mask for the address (for example, the value of 12 signifies that the BAR has a size of 4~KiB
and, therefore, the address bits [63:12] can be ignored). Other attributes of this header are either
reserved or used to create a response for \ac{MRd}.

% TODO: Explain, how the byte address can be acquired from the TUSER signal

For posted requests, such as \ac{MRd}, the response is dispatched on the \ac{CC} interface by using
the 3~DW TLP header as in Fig. XXX. The header may be followed by a payload. The \emph{Address} is a
byte-level address of the first byte of the memory block being transferred. Each completion
transferes a status of either \emph{Successful Completion}, \emph{Unsupported Request} or
\emph{Completer Abort} in the \emph{Completion Status} field. The size of the payload is specified
in the \emph{Dword Count} field. Similar to the Target Function field in the RQ header, there is an
index of a completing Function in the \emph{Completer ID} fields. Furthermore, there are multiple
fields that need to be copied from the Requester TLP header, such as \ac{AT}, \emph{Requester ID},
\emph{Tag}, \ac{TC} and \emph{Attributes} in order that the Requester correctly classifies the
received Completion. This applies especially for the Tag field which marks all Completions for a
specific Request. A Completion can be dispatch in multiple TLPs, especially if the value of MRRS

Since there is only one PCIe IP per one physical connector, all of the requests pass through same
interfaces to the user logic and have to be split according to the function index and the BAR ID.

% TODO: Picture of RQ header

% Reason why two Functions are used
